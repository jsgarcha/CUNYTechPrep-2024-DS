{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2024 Data Science Track: Week 2 - Data Cleaning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Packages, Packages!\n",
    "\n",
    "Import *all* the things here! You need the standard stuff: `pandas` and `numpy`.\n",
    "\n",
    "If you got more stuff you want to use, add them here too. 🙂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import here.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "With the packages out of the way, now you will be working with the following data sets:\n",
    "\n",
    "* `food_coded.csv`: [Food choices](https://www.kaggle.com/datasets/borapajo/food-choices?select=food_coded.csv) from Kaggle\n",
    "* `Ask A Manager Salary Survey 2021 (Responses) - Form Responses 1.tsv`: [Ask A Manager Salary Survey 2021 (Responses)](https://docs.google.com/spreadsheets/d/1IPS5dBSGtwYVbjsfbaMCYIWnOuRmJcbequohNxCyGVw/view?&gid=1625408792) as *Tab Separated Values (.tsv)* from Google Docs\n",
    "\n",
    "Each one poses different challenges. But you’ll―of course―overcome them with what you learned in class! 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Choices Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Food choices data set into a variable (e.g., df_food).\n",
    "\n",
    "food_data_set_path = 'data/food_coded.csv'\n",
    "df_food = pd.read_csv(food_data_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data did you just load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 61) 125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>calories_chicken</th>\n",
       "      <th>calories_day</th>\n",
       "      <th>calories_scone</th>\n",
       "      <th>coffee</th>\n",
       "      <th>comfort_food_reasons_coded</th>\n",
       "      <th>cook</th>\n",
       "      <th>comfort_food_reasons_coded.1</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>...</th>\n",
       "      <th>persian_food</th>\n",
       "      <th>self_perception_weight</th>\n",
       "      <th>soup</th>\n",
       "      <th>sports</th>\n",
       "      <th>thai_food</th>\n",
       "      <th>tortilla_calories</th>\n",
       "      <th>turkey_calories</th>\n",
       "      <th>veggies_day</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>waffle_calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>125.00000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.392000</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>577.320000</td>\n",
       "      <td>3.028302</td>\n",
       "      <td>505.241935</td>\n",
       "      <td>1.75200</td>\n",
       "      <td>2.698113</td>\n",
       "      <td>2.786885</td>\n",
       "      <td>2.688000</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>...</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>3.120968</td>\n",
       "      <td>1.217742</td>\n",
       "      <td>1.390244</td>\n",
       "      <td>3.336000</td>\n",
       "      <td>947.580645</td>\n",
       "      <td>555.040000</td>\n",
       "      <td>4.008000</td>\n",
       "      <td>1.512000</td>\n",
       "      <td>1073.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.490161</td>\n",
       "      <td>0.316636</td>\n",
       "      <td>131.214156</td>\n",
       "      <td>0.639308</td>\n",
       "      <td>230.840506</td>\n",
       "      <td>0.43359</td>\n",
       "      <td>1.972042</td>\n",
       "      <td>1.038351</td>\n",
       "      <td>1.910987</td>\n",
       "      <td>0.974759</td>\n",
       "      <td>...</td>\n",
       "      <td>1.423824</td>\n",
       "      <td>1.115980</td>\n",
       "      <td>0.414385</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>1.436528</td>\n",
       "      <td>202.090179</td>\n",
       "      <td>152.370379</td>\n",
       "      <td>1.081337</td>\n",
       "      <td>0.501867</td>\n",
       "      <td>248.667092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>575.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>725.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>610.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1165.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1315.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1165.000000</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1315.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Gender   breakfast  calories_chicken  calories_day  calories_scone  \\\n",
       "count  125.000000  125.000000        125.000000    106.000000      124.000000   \n",
       "mean     1.392000    1.112000        577.320000      3.028302      505.241935   \n",
       "std      0.490161    0.316636        131.214156      0.639308      230.840506   \n",
       "min      1.000000    1.000000        265.000000      2.000000      315.000000   \n",
       "25%      1.000000    1.000000        430.000000      3.000000      420.000000   \n",
       "50%      1.000000    1.000000        610.000000      3.000000      420.000000   \n",
       "75%      2.000000    1.000000        720.000000      3.000000      420.000000   \n",
       "max      2.000000    2.000000        720.000000      4.000000      980.000000   \n",
       "\n",
       "          coffee  comfort_food_reasons_coded        cook  \\\n",
       "count  125.00000                  106.000000  122.000000   \n",
       "mean     1.75200                    2.698113    2.786885   \n",
       "std      0.43359                    1.972042    1.038351   \n",
       "min      1.00000                    1.000000    1.000000   \n",
       "25%      2.00000                    2.000000    2.000000   \n",
       "50%      2.00000                    2.000000    3.000000   \n",
       "75%      2.00000                    3.000000    3.000000   \n",
       "max      2.00000                    9.000000    5.000000   \n",
       "\n",
       "       comfort_food_reasons_coded.1     cuisine  ...  persian_food  \\\n",
       "count                    125.000000  108.000000  ...    124.000000   \n",
       "mean                       2.688000    1.388889  ...      2.806452   \n",
       "std                        1.910987    0.974759  ...      1.423824   \n",
       "min                        1.000000    1.000000  ...      1.000000   \n",
       "25%                        2.000000    1.000000  ...      2.000000   \n",
       "50%                        2.000000    1.000000  ...      3.000000   \n",
       "75%                        3.000000    1.000000  ...      4.000000   \n",
       "max                        9.000000    6.000000  ...      5.000000   \n",
       "\n",
       "       self_perception_weight        soup      sports   thai_food  \\\n",
       "count              124.000000  124.000000  123.000000  125.000000   \n",
       "mean                 3.120968    1.217742    1.390244    3.336000   \n",
       "std                  1.115980    0.414385    0.489800    1.436528   \n",
       "min                  1.000000    1.000000    1.000000    1.000000   \n",
       "25%                  2.000000    1.000000    1.000000    2.000000   \n",
       "50%                  3.000000    1.000000    1.000000    3.000000   \n",
       "75%                  4.000000    1.000000    2.000000    5.000000   \n",
       "max                  6.000000    2.000000    2.000000    5.000000   \n",
       "\n",
       "       tortilla_calories  turkey_calories  veggies_day    vitamins  \\\n",
       "count         124.000000       125.000000   125.000000  125.000000   \n",
       "mean          947.580645       555.040000     4.008000    1.512000   \n",
       "std           202.090179       152.370379     1.081337    0.501867   \n",
       "min           580.000000       345.000000     1.000000    1.000000   \n",
       "25%           725.000000       500.000000     3.000000    1.000000   \n",
       "50%           940.000000       500.000000     4.000000    2.000000   \n",
       "75%          1165.000000       690.000000     5.000000    2.000000   \n",
       "max          1165.000000       850.000000     5.000000    2.000000   \n",
       "\n",
       "       waffle_calories  \n",
       "count       125.000000  \n",
       "mean       1073.400000  \n",
       "std         248.667092  \n",
       "min         575.000000  \n",
       "25%         900.000000  \n",
       "50%         900.000000  \n",
       "75%        1315.000000  \n",
       "max        1315.000000  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count by hand. (lol kidding)\n",
    "print(df_food.shape, len(df_food))\n",
    "df_food.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns and their types in this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125 entries, 0 to 124\n",
      "Data columns (total 61 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   GPA                           123 non-null    object \n",
      " 1   Gender                        125 non-null    int64  \n",
      " 2   breakfast                     125 non-null    int64  \n",
      " 3   calories_chicken              125 non-null    int64  \n",
      " 4   calories_day                  106 non-null    float64\n",
      " 5   calories_scone                124 non-null    float64\n",
      " 6   coffee                        125 non-null    int64  \n",
      " 7   comfort_food                  124 non-null    object \n",
      " 8   comfort_food_reasons          123 non-null    object \n",
      " 9   comfort_food_reasons_coded    106 non-null    float64\n",
      " 10  cook                          122 non-null    float64\n",
      " 11  comfort_food_reasons_coded.1  125 non-null    int64  \n",
      " 12  cuisine                       108 non-null    float64\n",
      " 13  diet_current                  124 non-null    object \n",
      " 14  diet_current_coded            125 non-null    int64  \n",
      " 15  drink                         123 non-null    float64\n",
      " 16  eating_changes                122 non-null    object \n",
      " 17  eating_changes_coded          125 non-null    int64  \n",
      " 18  eating_changes_coded1         125 non-null    int64  \n",
      " 19  eating_out                    125 non-null    int64  \n",
      " 20  employment                    116 non-null    float64\n",
      " 21  ethnic_food                   125 non-null    int64  \n",
      " 22  exercise                      112 non-null    float64\n",
      " 23  father_education              124 non-null    float64\n",
      " 24  father_profession             122 non-null    object \n",
      " 25  fav_cuisine                   123 non-null    object \n",
      " 26  fav_cuisine_coded             125 non-null    int64  \n",
      " 27  fav_food                      123 non-null    float64\n",
      " 28  food_childhood                124 non-null    object \n",
      " 29  fries                         125 non-null    int64  \n",
      " 30  fruit_day                     125 non-null    int64  \n",
      " 31  grade_level                   125 non-null    int64  \n",
      " 32  greek_food                    125 non-null    int64  \n",
      " 33  healthy_feeling               125 non-null    int64  \n",
      " 34  healthy_meal                  124 non-null    object \n",
      " 35  ideal_diet                    124 non-null    object \n",
      " 36  ideal_diet_coded              125 non-null    int64  \n",
      " 37  income                        124 non-null    float64\n",
      " 38  indian_food                   125 non-null    int64  \n",
      " 39  italian_food                  125 non-null    int64  \n",
      " 40  life_rewarding                124 non-null    float64\n",
      " 41  marital_status                124 non-null    float64\n",
      " 42  meals_dinner_friend           122 non-null    object \n",
      " 43  mother_education              122 non-null    float64\n",
      " 44  mother_profession             123 non-null    object \n",
      " 45  nutritional_check             125 non-null    int64  \n",
      " 46  on_off_campus                 124 non-null    float64\n",
      " 47  parents_cook                  125 non-null    int64  \n",
      " 48  pay_meal_out                  125 non-null    int64  \n",
      " 49  persian_food                  124 non-null    float64\n",
      " 50  self_perception_weight        124 non-null    float64\n",
      " 51  soup                          124 non-null    float64\n",
      " 52  sports                        123 non-null    float64\n",
      " 53  thai_food                     125 non-null    int64  \n",
      " 54  tortilla_calories             124 non-null    float64\n",
      " 55  turkey_calories               125 non-null    int64  \n",
      " 56  type_sports                   99 non-null     object \n",
      " 57  veggies_day                   125 non-null    int64  \n",
      " 58  vitamins                      125 non-null    int64  \n",
      " 59  waffle_calories               125 non-null    int64  \n",
      " 60  weight                        123 non-null    object \n",
      "dtypes: float64(20), int64(27), object(14)\n",
      "memory usage: 59.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show the column names and their types.\n",
    "df_food.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "Perhaps we’d like to know more another day, but the team is really interested in just the relationship between calories (`calories_day`) and weight. …and maybe gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you remove the other columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df_food: \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalories_day\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m column \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m column \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         df_food\u001b[38;5;241m.\u001b[39mdrop(column, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Remove ‘em.\n",
    "for column in df_food: \n",
    "    if column != 'calories_day' or column != 'weight' or column != 'gender':\n",
    "        df_food.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about `NaN`s? How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPA                  2\n",
       "Gender               0\n",
       "breakfast            0\n",
       "calories_chicken     0\n",
       "calories_day        19\n",
       "                    ..\n",
       "type_sports         26\n",
       "veggies_day          0\n",
       "vitamins             0\n",
       "waffle_calories      0\n",
       "weight               2\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count ‘em.\n",
    "df_food.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gotta remove those `NaN`s―the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ‘em.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the weird non-numeric values in the column obviously meant for numeric data?\n",
    "\n",
    "Notice the data type of that column from when you got the types of all the columns?\n",
    "\n",
    "If only we could convert the column to a numeric type and drop the rows with invalid values. 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this data seems reasonably clean for our purposes! 😁\n",
    "\n",
    "Let’s save it somewhere to be shipped off to another teammate. 💾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savey save!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Manager Salary Survey 2021 (Responses) Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 1142, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_salary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/manager_salary_survey_2021.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 1142, saw 12\n"
     ]
    }
   ],
   "source": [
    "df_salary = pd.read_csv('data/manager_salary_survey_2021.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was that hard? 🙃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rename the file to something that is better for all systems.  \n",
    "* No spaces in filename (can use '_')\n",
    "* all lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore\n",
    "\n",
    "You know the drill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data did you just load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by hand. I’m dead serious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns and their types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the column names and their types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh… Ugh! Give these columns easier names to work with first. 🙄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename ‘em.\n",
    "# Non-binding suggestions: timestamp, age, industry, title, title_context, salary, additional_compensation, currency, other_currency, salary_context, country, state, city, total_yoe, field_yoe, highest_education_completed\tgender, race\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a lot, and that should not have been easy. 😏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re going to have a gander at the computing/tech subset first because thats *your* industry. But first, what value corresponds to that `industry`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the unique industries and a count of their instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That value among the top 5 is what you’re looking for innit? Filter out all the rows not in that industry and save it into a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtery filter. (Save it to a new variable, df_salary_tech.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a sanity check to make sure that the only values you kept are the one you are filtered for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are very interested in salary figures. But how many dollars 💵 is a euro 💶 or a pound 💷? That sounds like a problem for another day. 🫠\n",
    "\n",
    "For now, let’s just look at U.S. dollars (`'USD'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtery filter for just the jobs that pay in USD!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want know is how each U.S. city pays in tech. What value in `country` represents the United States of America?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did filter for USD, so if we do a count of each unique country in descending count order, the relevant value(s) should show up at the top.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "Well, we can’t get our answers with what we currently have, so you’ll have to make some changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s not worry about anything below the first 5 values for now. Convert the top 5 to a single canonical value―say, `'US'`, which is nice and short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace them all with 'US'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the count of each unique country again now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CREDIT: resolve [most of] those anomalous cases too without exhaustively taking every variant literally into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BONUS CREDIT: if you’ve resolved it, let’s see how well you did by counting the number of instances of each unique value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s looking good so far. Let’s find out the minimum, mean, and maximum (in that order) salary by state, sorted by the mean in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum, mean, and maximum salary in USD by U.S. state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, pooh! We forgot that `salary` isn’t numeric. Something wrong must be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it again. Yeah!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did the trick! Now let’s narrow this to data 2021 and 2022 just because (lel). *(Hint: that timestamp column may not be a temporal type right now.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to within 2021, 2022, or 2023, saving the DataFrame to a new variable, and generate the summary again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Clearly, we do not have enough data to produce useful figures for the level of specificity you’ve now reached. What do you notice about Delaware and West Virginia?\n",
    "\n",
    "Let’s back out a bit and return to `df_salary` (which was the loaded data with renamed columns but *sans* filtering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #0\n",
    "\n",
    "Apply the same steps as before to `df_salary`, but do not filter for any specific industry. Do perform the other data cleaning stuff, and get to a point where you can generate the minimum, mean, and maximum by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #1\n",
    "\n",
    "This time, format the table output nicely (*$12,345.00*) without modifying the values in the `DataFrame`. That is, `df_salary` should be identical before versus after running your code.\n",
    "\n",
    "(*Hint: if you run into an error about `jinja2` perhaps you need to `pip install` something.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #2\n",
    "\n",
    "Filter out the non-single-states (e.g., `'California, Colorado'`) in the most elegant way possible (i.e., *not* by blacklisting all the bad values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #3\n",
    "\n",
    "Show the quantiles instead of just minimum, mean, and maximum―say 0%, 5%, 25%, 50%, 75%, 95%, and 100%. Outliers may be deceiving.\n",
    "\n",
    "Sort by whatever interests you―like say the *50th* percentile.\n",
    "\n",
    "And throw in a count by state too. It would be interesting to know how many data points contribute to the figures for each state. (*Hint: your nice formatting from Bonus #1 might not work this time around.* 😜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
